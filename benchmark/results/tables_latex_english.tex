% Evaluation Metrics Tables (English Version)

% MedQA Evaluation Metrics
\begin{table}[H]
  \centering
  \caption{Evaluation Metrics Comparison - MedQA Dataset}
  \label{tab:medqa_metrics}
  \begin{tabular}{lccccc}
    \toprule
    Method & ROUGE-1 & ROUGE-2 & ROUGE-L & BLEU & BERTScore F1 \\
    \midrule
    Baseline & 0.0338 & 0.0180 & 0.0331 & 0.0064 & 0.7843 \\
    Depth=1 & 0.3185 & 0.2204 & 0.3155 & 0.1144 & 0.8656 \\
    Depth=2 & 0.3080 & 0.2007 & 0.3058 & 0.1148 & 0.8572 \\
    Depth=3 & 0.3101 & 0.1976 & 0.3080 & 0.1065 & 0.8611 \\
    \bottomrule
  \end{tabular}
\end{table}

% DART Evaluation Metrics
\begin{table}[H]
  \centering
  \caption{Evaluation Metrics Comparison - DART Dataset}
  \label{tab:dart_metrics}
  \begin{tabular}{lccccc}
    \toprule
    Method & ROUGE-1 & ROUGE-2 & ROUGE-L & BLEU & BERTScore F1 \\
    \midrule
    Baseline & 0.4889 & 0.3531 & 0.4195 & 0.2222 & 0.9004 \\
    Depth=1 & 0.7292 & 0.5400 & 0.6003 & 0.3805 & 0.9328 \\
    Depth=2 & 0.7351 & 0.5399 & 0.6070 & 0.3861 & 0.9335 \\
    Depth=3 & 0.7330 & 0.5303 & 0.6021 & 0.3751 & 0.9335 \\
    \bottomrule
  \end{tabular}
\end{table}

% TriviaQA Evaluation Metrics
\begin{table}[H]
  \centering
  \caption{Evaluation Metrics Comparison - TriviaQA Dataset}
  \label{tab:triviaqa_metrics}
  \begin{tabular}{lccccc}
    \toprule
    Method & ROUGE-1 & ROUGE-2 & ROUGE-L & BLEU & BERTScore F1 \\
    \midrule
    Baseline & 0.0353 & 0.0091 & 0.0353 & 0.0032 & 0.8106 \\
    Depth=1 & 0.6700 & 0.2798 & 0.6700 & 0.1535 & 0.9290 \\
    Depth=2 & 0.6949 & 0.2798 & 0.6949 & 0.1527 & 0.9311 \\
    Depth=3 & 0.6848 & 0.2859 & 0.6848 & 0.1496 & 0.9259 \\
    \bottomrule
  \end{tabular}
\end{table}

% Time Performance Tables

% MedQA Time Performance
\begin{table}[H]
  \centering
  \caption{Time Performance Comparison - MedQA Dataset}
  \label{tab:medqa_time}
  \begin{tabular}{lccc}
    \toprule
    Method & Avg Total Time (s) & Avg Retrieval Time (s) & Avg Generation Time (s) \\
    \midrule
    Baseline & 41.21 & 0.263 & 41.18 \\
    Depth=1 & 15.36 & 0.075 & 15.33 \\
    Depth=2 & 15.42 & 0.041 & 15.40 \\
    Depth=3 & 14.56 & 0.041 & 14.54 \\
    \bottomrule
  \end{tabular}
\end{table}

% DART Time Performance
\begin{table}[H]
  \centering
  \caption{Time Performance Comparison - DART Dataset}
  \label{tab:dart_time}
  \begin{tabular}{lccc}
    \toprule
    Method & Avg Total Time (s) & Avg Retrieval Time (s) & Avg Generation Time (s) \\
    \midrule
    Baseline & 17.82 & 0.335 & 17.80 \\
    Depth=1 & 7.20 & 0.038 & 7.17 \\
    Depth=2 & 6.49 & 0.023 & 6.47 \\
    Depth=3 & 6.07 & 0.024 & 6.06 \\
    \bottomrule
  \end{tabular}
\end{table}

% TriviaQA Time Performance
\begin{table}[H]
  \centering
  \caption{Time Performance Comparison - TriviaQA Dataset}
  \label{tab:triviaqa_time}
  \begin{tabular}{lccc}
    \toprule
    Method & Avg Total Time (s) & Avg Retrieval Time (s) & Avg Generation Time (s) \\
    \midrule
    Baseline & 19.09 & 5.621 & 19.07 \\
    Depth=1 & 10.43 & 0.022 & 10.41 \\
    Depth=2 & 7.40 & 0.013 & 7.38 \\
    Depth=3 & 7.74 & 0.013 & 7.72 \\
    \bottomrule
  \end{tabular}
\end{table}

