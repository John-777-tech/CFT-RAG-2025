#!/usr/bin/env python
"""ä½¿ç”¨è®ºæ–‡æ•°æ®é›†è¿è¡Œbenchmarkæµ‹è¯•
æ”¯æŒMedQA, AESLC, DARTæ•°æ®é›†
"""

import sys
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from rag_base.build_index import load_vec_db
from rag_base.rag_complete import rag_complete
from trag_tree import build, hash

load_dotenv()


class BenchmarkRunner:
    """Benchmarkæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, vec_db_key: str, tree_num_max: int = 50, 
                 entities_file_name: str = "entities_file",
                 search_method: int = 2, node_num_max: int = 2000000):
        self.vec_db_key = vec_db_key
        self.tree_num_max = tree_num_max
        self.entities_file_name = entities_file_name
        self.search_method = search_method
        self.node_num_max = node_num_max
        
        print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“å’Œå®ä½“æ ‘...")
        start_time = time.time()
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        self.vec_db = load_vec_db(vec_db_key, "vec_db_cache/")
        print(f"âœ“ Vector DBåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # æ„å»ºforestå’Œnlp
        self.forest, self.nlp = build.build_forest(
            tree_num_max, entities_file_name, search_method, node_num_max
        )
        print(f"âœ“ Forestå’ŒNLPåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # æ ¹æ®search_methodæ‰§è¡Œä¸åŒçš„åˆå§‹åŒ–
        if search_method in [4, 8]:
            for entity_tree in self.forest:
                entity_tree.bfs_hash()
        
        if search_method in [9]:
            from grag_graph.graph import build_graph
            build_graph(entities_file_name)
        
        if search_method in [8, 9]:
            from ann.ann_calc import build_ann
            build_ann()
        
        if search_method in [7]:
            hash.cuckoo_build(tree_num_max, node_num_max)
        
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}ç§’)\n")
    
    def evaluate(self, question: str, expected_answer: str = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        start_time = time.time()
        
        # è·å–å›ç­”
        stream = rag_complete(
            question,
            self.vec_db,
            self.forest,
            self.nlp,
            search_method=self.search_method,
            debug=False,
        )
        
        answer = ""
        for chunk in stream:
            answer += chunk
        
        elapsed_time = time.time() - start_time
        
        return {
            "question": question,
            "answer": answer,
            "expected_answer": expected_answer,
            "time": elapsed_time,
            "answer_length": len(answer)
        }
    
    def run_dataset(self, dataset: List[Dict[str, str]], max_samples: int = None, 
                   checkpoint_path: str = None, resume: bool = True) -> List[Dict[str, Any]]:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡Œbenchmarkï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        
        Args:
            dataset: æ•°æ®é›†
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¿å­˜å’Œæ¢å¤è¿›åº¦ï¼‰
            resume: æ˜¯å¦ä»checkpointæ¢å¤
        """
        if max_samples:
            dataset = dataset[:max_samples]
        
        # å°è¯•ä»checkpointæ¢å¤
        completed_questions = set()
        results = []
        start_idx = 0
        
        if resume and checkpoint_path and os.path.exists(checkpoint_path):
            try:
                print(f"æ­£åœ¨ä»checkpointæ¢å¤: {checkpoint_path}")
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    if isinstance(checkpoint_data, list):
                        results = checkpoint_data
                        # è®°å½•å·²å®Œæˆçš„é—®é¢˜ï¼ˆä½¿ç”¨é—®é¢˜æ–‡æœ¬çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                        completed_questions = {r['question'][:100] for r in results if r.get('question')}
                        start_idx = len(results)
                        print(f"âœ“ ä»checkpointæ¢å¤äº† {len(results)} ä¸ªå·²å®Œæˆçš„ç»“æœ")
            except Exception as e:
                print(f"âš  è¯»å–checkpointå¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹è¿è¡Œ")
                results = []
                completed_questions = set()
                start_idx = 0
        
        total_start = time.time()
        
        print(f"\nå¼€å§‹æµ‹è¯• {len(dataset)} ä¸ªé—®é¢˜...")
        if start_idx > 0:
            print(f"å·²å®Œæˆ {start_idx} ä¸ªï¼Œå‰©ä½™ {len(dataset) - start_idx} ä¸ª")
        print("=" * 80)
        
        checkpoint_interval = 10  # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡checkpoint
        
        for i, item in enumerate(dataset[start_idx:], start_idx + 1):
            question = item.get("prompt", item.get("question", ""))
            expected = item.get("answer", item.get("expected_answer", ""))
            
            if not question:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆé¿å…é‡å¤è¿è¡Œï¼‰
            question_key = question[:100]
            if question_key in completed_questions:
                print(f"\n[{i}/{len(dataset)}] â­ è·³è¿‡å·²å®Œæˆçš„: {question[:60]}...")
                continue
            
            print(f"\n[{i}/{len(dataset)}] {question[:60]}...")
            
            try:
                result = self.evaluate(question, expected)
                results.append(result)
                completed_questions.add(question_key)
                
                print(f"  å›ç­”é•¿åº¦: {len(result['answer'])} å­—ç¬¦")
                print(f"  è€—æ—¶: {result['time']:.2f}ç§’")
                
                # å®šæœŸä¿å­˜checkpoint
                if checkpoint_path and i % checkpoint_interval == 0:
                    try:
                        os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                        with open(checkpoint_path, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
                        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜ ({i}/{len(dataset)})")
                    except Exception as e:
                        print(f"  âš  Checkpointä¿å­˜å¤±è´¥: {e}")
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼Œä½†æ ‡è®°ä¸ºå¤±è´¥
                results.append({
                    "question": question,
                    "answer": f"[ERROR: {str(e)}]",
                    "expected_answer": expected,
                    "time": 0,
                    "answer_length": 0,
                    "error": str(e)
                })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        
        total_time = time.time() - total_start
        
        # æœ€ç»ˆä¿å­˜checkpoint
        if checkpoint_path:
            try:
                os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                with open(checkpoint_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ æœ€ç»ˆcheckpointå·²ä¿å­˜: {checkpoint_path}")
            except Exception as e:
                print(f"âš  æœ€ç»ˆcheckpointä¿å­˜å¤±è´¥: {e}")
        
        print("\n" + "=" * 80)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        print(f"æ€»é—®é¢˜æ•°: {len(results)}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        if results:
            avg_time = sum(r['time'] for r in results) / len(results)
            avg_length = sum(r['answer_length'] for r in results) / len(results)
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print("=" * 80)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def load_json_dataset(file_path: str) -> List[Dict[str, str]]:
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆåŒ…å«å¤šä¸ªæ•°æ®é›†ï¼‰ï¼Œæå–ç¬¬ä¸€ä¸ª
    if isinstance(data, dict):
        # å°è¯•æ‰¾åˆ°åˆ—è¡¨æ ¼å¼çš„æ•°æ®
        for key, value in data.items():
            if isinstance(value, list):
                return value
        return []
    
    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if isinstance(data, list):
        return data
    
    return []


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¿è¡Œbenchmarkæµ‹è¯•")
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vec-db-key', type=str, default="test",
                       help='å‘é‡æ•°æ®åº“key')
    parser.add_argument('--tree-num-max', type=int, default=50,
                       help='æœ€å¤§æ ‘æ•°é‡')
    parser.add_argument('--entities-file-name', type=str, default="entities_file",
                       help='å®ä½“æ–‡ä»¶åï¼ˆä¸å«.csvæ‰©å±•åï¼‰')
    parser.add_argument('--search-method', type=int, default=2,
                       choices=[0, 1, 2, 5, 7, 8, 9],
                       help='æœç´¢æ–¹æ³•')
    parser.add_argument('--node-num-max', type=int, default=2000000,
                       help='æœ€å¤§èŠ‚ç‚¹æ•°')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='ç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='Checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼Œé»˜è®¤ä¸outputç›¸åŒï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»checkpointæ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†: {args.dataset}")
    dataset = load_json_dataset(args.dataset)
    
    if not dataset:
        print(f"âœ— æ— æ³•åŠ è½½æ•°æ®é›†: {args.dataset}")
        return
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®\n")
    
    # åˆ›å»ºrunner
    runner = BenchmarkRunner(
        vec_db_key=args.vec_db_key,
        tree_num_max=args.tree_num_max,
        entities_file_name=args.entities_file_name,
        search_method=args.search_method,
        node_num_max=args.node_num_max
    )
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„å’Œcheckpointè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºè·¯å¾„
        dataset_name = Path(args.dataset).stem
        output_path = f"./benchmark/results/{dataset_name}_results_{args.search_method}.json"
    
    checkpoint_path = args.checkpoint if args.checkpoint else output_path
    resume = not args.no_resume
    
    # è¿è¡Œæµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    results = runner.run_dataset(
        dataset, 
        max_samples=args.max_samples,
        checkpoint_path=checkpoint_path,
        resume=resume
    )
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    runner.save_results(results, output_path)


if __name__ == "__main__":
    main()

