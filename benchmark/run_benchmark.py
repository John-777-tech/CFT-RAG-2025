#!/usr/bin/env python
"""ä½¿ç”¨è®ºæ–‡æ•°æ®é›†è¿è¡Œbenchmarkæµ‹è¯•
æ”¯æŒMedQA, AESLC, DARTæ•°æ®é›†
"""

import sys
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from rag_base.build_index import load_vec_db
from rag_base.rag_complete import rag_complete
from trag_tree import build, hash

load_dotenv()


class BenchmarkRunner:
    """Benchmarkæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, vec_db_key: str, tree_num_max: int = 50, 
                 entities_file_name: str = "entities_file",
                 search_method: int = 2, node_num_max: int = 2000000):
        self.vec_db_key = vec_db_key
        self.tree_num_max = tree_num_max
        self.entities_file_name = entities_file_name
        self.search_method = search_method
        self.node_num_max = node_num_max
        
        print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“å’Œå®ä½“æ ‘...")
        start_time = time.time()
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        # æ ¹æ®vec_db_keyç¡®å®šæ•°æ®æºè·¯å¾„
        if vec_db_key == "medqa":
            data_source = "/Users/zongyikun/Downloads/Med_data_clean/textbooks/en"
        elif vec_db_key == "dart":
            data_source = "/Users/zongyikun/Downloads/dart-v1.1.1-full-dev.json"
        elif vec_db_key == "aeslc":
            # AESLCæ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨answerå­—æ®µä½œä¸ºchunksï¼‰
            data_source = "./datasets/processed/aeslc.json"
        elif vec_db_key == "triviaqa":
            # TriviaQAæ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨answerå­—æ®µä½œä¸ºchunksï¼‰
            data_source = "./datasets/processed/triviaqa.json"
        else:
            # é»˜è®¤å°è¯•ä»vec_db_cacheåŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“
            data_source = "vec_db_cache/"
        
        self.vec_db = load_vec_db(vec_db_key, data_source)
        print(f"âœ“ Vector DBåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # search_method=0 (baseline RAG) ä¸éœ€è¦forestå’Œnlp
        if search_method == 0:
            self.forest = None
            self.nlp = None
            print(f"âœ“ Baseline RAGæ¨¡å¼ï¼Œè·³è¿‡Forestå’ŒNLPæ„å»º ({time.time() - start_time:.2f}ç§’)")
        else:
            # æ„å»ºforestå’Œnlp
            self.forest, self.nlp = build.build_forest(
                tree_num_max, entities_file_name, search_method, node_num_max
            )
            print(f"âœ“ Forestå’ŒNLPåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
            
            # æ ¹æ®search_methodæ‰§è¡Œä¸åŒçš„åˆå§‹åŒ–
            if search_method in [4, 8]:
                for entity_tree in self.forest:
                    entity_tree.bfs_hash()
            
            if search_method in [9]:
                from grag_graph.graph import build_graph
                build_graph(entities_file_name)
            
            if search_method in [8, 9]:
                from ann.ann_calc import build_ann
                build_ann()
            
            # Cuckoo filter (search_method == 7) - initialize cuckoo filter if needed
            if search_method == 7:
                # Check if filter is already initialized (from build_forest)
                # If not, initialize it now
                if hash.filter is None:
                    # First initialize the filter (change_filter must be called before cuckoo_build)
                    estimated_entities = 100000  # Safe default
                    try:
                        # Try to get actual entity count from forest if available
                        if self.forest:
                            total_entities = sum(len(tree.all_nodes) for tree in self.forest if hasattr(tree, 'all_nodes'))
                            if total_entities > 0:
                                estimated_entities = max(total_entities, 10000)
                    except:
                        pass
                    hash.change_filter(estimated_entities)
                
                # Note: cuckoo_build reads from "new_entities_file.csv" which may not exist
                # The enhanced search function doesn't strictly require cuckoo_build
                # So we'll skip it for now and use the filter directly
                # If needed, cuckoo_build can be called separately after ensuring the file exists
                print(f"âœ“ Cuckoo Filterå·²åˆå§‹åŒ– (è·³è¿‡buildæ­¥éª¤ï¼Œä½¿ç”¨å¢å¼ºæœç´¢å‡½æ•°)")
        
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}ç§’)\n")
    
    def evaluate(self, question: str, expected_answer: str = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        start_time = time.time()
        
        # å¯¼å…¥rag_completeæ¨¡å—ä»¥è·å–retrieval_timeå’Œgeneration_time
        from rag_base.rag_complete import get_retrieval_time, get_generation_time
        
        # è·å–å›ç­”
        stream = rag_complete(
            question,
            self.vec_db,
            self.forest,
            self.nlp,
            search_method=self.search_method,
            debug=False,
        )
        
        answer = ""
        for chunk in stream:
            answer += chunk
        
        elapsed_time = time.time() - start_time
        
        # è·å–æ£€ç´¢æ—¶é—´å’Œç”Ÿæˆæ—¶é—´
        retrieval_time = get_retrieval_time()
        generation_time = get_generation_time()
        
        result = {
            "question": question,
            "answer": answer,
            "expected_answer": expected_answer,
            "time": elapsed_time,
            "answer_length": len(answer)
        }
        
        # å¦‚æœæ£€ç´¢æ—¶é—´å’Œç”Ÿæˆæ—¶é—´å¯ç”¨ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if retrieval_time is not None:
            result["retrieval_time"] = retrieval_time
        if generation_time is not None:
            result["generation_time"] = generation_time
        
        return result
    
    def run_dataset(self, dataset: List[Dict[str, str]], max_samples: int = None, 
                   checkpoint_path: str = None, resume: bool = True) -> List[Dict[str, Any]]:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡Œbenchmarkï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        
        Args:
            dataset: æ•°æ®é›†
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¿å­˜å’Œæ¢å¤è¿›åº¦ï¼‰
            resume: æ˜¯å¦ä»checkpointæ¢å¤
        """
        if max_samples:
            dataset = dataset[:max_samples]
        
        # å°è¯•ä»checkpointæ¢å¤
        completed_questions = set()
        results = []
        start_idx = 0
        
        if resume and checkpoint_path and os.path.exists(checkpoint_path):
            try:
                print(f"æ­£åœ¨ä»checkpointæ¢å¤: {checkpoint_path}")
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    if isinstance(checkpoint_data, list):
                        results = checkpoint_data
                        # è®°å½•å·²å®Œæˆçš„é—®é¢˜ï¼ˆä½¿ç”¨å®Œæ•´questionä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                        completed_questions = {r['question'] for r in results if r.get('question')}
                        start_idx = len(results)
                        print(f"âœ“ ä»checkpointæ¢å¤äº† {len(results)} ä¸ªå·²å®Œæˆçš„ç»“æœ")
            except Exception as e:
                print(f"âš  è¯»å–checkpointå¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹è¿è¡Œ")
                results = []
                completed_questions = set()
                start_idx = 0
        
        total_start = time.time()
        
        print(f"\nå¼€å§‹æµ‹è¯• {len(dataset)} ä¸ªé—®é¢˜...")
        if start_idx > 0:
            print(f"å·²å®Œæˆ {start_idx} ä¸ªï¼Œå‰©ä½™ {len(dataset) - start_idx} ä¸ª")
        print("=" * 80)
        
        checkpoint_interval = 10  # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡checkpoint
        
        for i, item in enumerate(dataset[start_idx:], start_idx + 1):
            question = item.get("prompt", item.get("question", ""))
            expected = item.get("answer", item.get("expected_answer", ""))
            
            if not question:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆé¿å…é‡å¤è¿è¡Œï¼‰
            # ä½¿ç”¨å®Œæ•´questionä½œä¸ºæ ‡è¯†ï¼Œå› ä¸ºå¾ˆå¤šé—®é¢˜çš„å‰100å­—ç¬¦å¯èƒ½ç›¸åŒ
            question_key = question  # ä½¿ç”¨å®Œæ•´questionè€Œä¸æ˜¯å‰100å­—ç¬¦
            if question_key in completed_questions:
                print(f"\n[{i}/{len(dataset)}] â­ è·³è¿‡å·²å®Œæˆçš„: {question[:60]}...")
                continue
            
            print(f"\n[{i}/{len(dataset)}] {question[:60]}...")
            
            try:
                result = self.evaluate(question, expected)
                results.append(result)
                completed_questions.add(question_key)
                
                print(f"  å›ç­”é•¿åº¦: {len(result['answer'])} å­—ç¬¦")
                print(f"  è€—æ—¶: {result['time']:.2f}ç§’")
                
                # å®šæœŸä¿å­˜checkpoint
                if checkpoint_path and i % checkpoint_interval == 0:
                    try:
                        os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                        with open(checkpoint_path, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
                        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜ ({i}/{len(dataset)})")
                    except Exception as e:
                        print(f"  âš  Checkpointä¿å­˜å¤±è´¥: {e}")
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼Œä½†æ ‡è®°ä¸ºå¤±è´¥
                results.append({
                    "question": question,
                    "answer": f"[ERROR: {str(e)}]",
                    "expected_answer": expected,
                    "time": 0,
                    "answer_length": 0,
                    "error": str(e)
                })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        
        total_time = time.time() - total_start
        
        # æœ€ç»ˆä¿å­˜checkpoint
        if checkpoint_path:
            try:
                os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                with open(checkpoint_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ æœ€ç»ˆcheckpointå·²ä¿å­˜: {checkpoint_path}")
            except Exception as e:
                print(f"âš  æœ€ç»ˆcheckpointä¿å­˜å¤±è´¥: {e}")
        
        print("\n" + "=" * 80)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        print(f"æ€»é—®é¢˜æ•°: {len(results)}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        if results:
            avg_time = sum(r['time'] for r in results) / len(results)
            avg_length = sum(r['answer_length'] for r in results) / len(results)
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print("=" * 80)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def load_json_dataset(file_path: str) -> List[Dict[str, str]]:
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆåŒ…å«å¤šä¸ªæ•°æ®é›†ï¼‰ï¼Œæå–ç¬¬ä¸€ä¸ª
    if isinstance(data, dict):
        # å°è¯•æ‰¾åˆ°åˆ—è¡¨æ ¼å¼çš„æ•°æ®
        for key, value in data.items():
            if isinstance(value, list):
                return value
        return []
    
    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if isinstance(data, list):
        return data
    
    return []


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¿è¡Œbenchmarkæµ‹è¯•")
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vec-db-key', type=str, default="test",
                       help='å‘é‡æ•°æ®åº“key')
    parser.add_argument('--tree-num-max', type=int, default=50,
                       help='æœ€å¤§æ ‘æ•°é‡')
    parser.add_argument('--entities-file-name', type=str, default="entities_file",
                       help='å®ä½“æ–‡ä»¶åï¼ˆä¸å«.csvæ‰©å±•åï¼‰')
    parser.add_argument('--search-method', type=int, default=0,
                       choices=[0, 1, 2, 5, 7, 8, 9],
                       help='æœç´¢æ–¹æ³•: 0 for vec-db only (standard RAG), 1 for BFS, 2 for BloomFilter, 5 for improved BloomFilter, 7 for Cuckoo Filter, 8 for ANN-Tree, 9 for ANN-Graph')
    parser.add_argument('--node-num-max', type=int, default=2000000,
                       help='æœ€å¤§èŠ‚ç‚¹æ•°')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='ç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='Checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼Œé»˜è®¤ä¸outputç›¸åŒï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»checkpointæ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†: {args.dataset}")
    dataset = load_json_dataset(args.dataset)
    
    if not dataset:
        print(f"âœ— æ— æ³•åŠ è½½æ•°æ®é›†: {args.dataset}")
        return
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®\n")
    
    # åˆ›å»ºrunner
    runner = BenchmarkRunner(
        vec_db_key=args.vec_db_key,
        tree_num_max=args.tree_num_max,
        entities_file_name=args.entities_file_name,
        search_method=args.search_method,
        node_num_max=args.node_num_max
    )
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„å’Œcheckpointè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºè·¯å¾„
        dataset_name = Path(args.dataset).stem
        output_path = f"./benchmark/results/{dataset_name}_results_{args.search_method}.json"
    
    checkpoint_path = args.checkpoint if args.checkpoint else output_path
    resume = not args.no_resume
    
    # è¿è¡Œæµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    results = runner.run_dataset(
        dataset, 
        max_samples=args.max_samples,
        checkpoint_path=checkpoint_path,
        resume=resume
    )
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    runner.save_results(results, output_path)


if __name__ == "__main__":
    main()

