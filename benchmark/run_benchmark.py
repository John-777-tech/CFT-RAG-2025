#!/usr/bin/env python
"""ä½¿ç”¨è®ºæ–‡æ•°æ®é›†è¿è¡Œbenchmarkæµ‹è¯•
æ”¯æŒMedQA, AESLC, DARTæ•°æ®é›†
"""

import sys
import json
import os
import time
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from rag_base.build_index import load_vec_db
from rag_base.rag_complete import rag_complete
from trag_tree import build, hash

load_dotenv()


class BenchmarkRunner:
    """Benchmarkæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, vec_db_key: str, tree_num_max: int = 50, 
                 entities_file_name: str = "entities_file",
                 search_method: int = 2, node_num_max: int = 2000000,
                 max_hierarchy_depth: int = 2):
        self.vec_db_key = vec_db_key
        self.tree_num_max = tree_num_max
        self.entities_file_name = entities_file_name
        self.search_method = search_method
        self.node_num_max = node_num_max
        self.max_hierarchy_depth = max_hierarchy_depth
        
        # æ ¹æ®search_methodæ˜¾ç¤ºä¸åŒçš„åŠ è½½ä¿¡æ¯
        if search_method == 7:
            print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“å’ŒCuckoo Filterï¼ˆæ–°æ¶æ„ï¼Œä¸ä½¿ç”¨å®ä½“æ ‘ï¼‰...")
        else:
            print("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“å’Œå®ä½“æ ‘...")
        start_time = time.time()
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        # æ ¹æ®vec_db_keyç¡®å®šæ•°æ®æºè·¯å¾„
        # ä½¿ç”¨æ–°çš„chunksæ–‡ä»¶æ„å»ºå‘é‡æ•°æ®åº“
        if vec_db_key == "medqa":
            data_source = "./extracted_data/medqa_chunks.txt"
        elif vec_db_key == "dart":
            data_source = "./extracted_data/dart_chunks.txt"
        elif vec_db_key == "aeslc":
            # AESLCæ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨answerå­—æ®µä½œä¸ºchunksï¼‰
            data_source = "./datasets/processed/aeslc.json"
        elif vec_db_key == "triviaqa":
            # TriviaQAæ•°æ®é›†è·¯å¾„ï¼ˆä½¿ç”¨æ–°çš„chunksæ–‡ä»¶ï¼‰
            data_source = "./extracted_data/triviaqa_chunks.txt"
        else:
            # é»˜è®¤å°è¯•ä»vec_db_cacheåŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“
            data_source = "vec_db_cache/"
        
        self.vec_db = load_vec_db(vec_db_key, data_source)
        print(f"âœ“ Vector DBåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
        
        # search_method=0 (baseline RAG) ä¸éœ€è¦forestå’Œnlp
        if search_method == 0:
            self.forest = None
            self.nlp = None
            print(f"âœ“ Baseline RAGæ¨¡å¼ï¼Œè·³è¿‡Forestå’ŒNLPæ„å»º ({time.time() - start_time:.2f}ç§’)")
        elif search_method == 7:
            # Cuckoo Filter (search_method == 7) - ä¸éœ€è¦æ„å»ºå®ä½“æ ‘ï¼Œåªéœ€è¦åŠ è½½å®ä½“åˆ—è¡¨å’Œåˆå§‹åŒ–spacy
            self.forest = None  # ä¸ä½¿ç”¨å®ä½“æ ‘
            
            # ä»å®ä½“æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰å®ä½“
            import csv
            entities_list = []
            entities_file_path_csv = f"{entities_file_name}.csv"
            entities_file_path_txt = f"{entities_file_name}.txt"
            
            # é¦–å…ˆå°è¯•ä»txtæ–‡ä»¶è¯»å–ï¼ˆå®ä½“åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªå®ä½“ï¼‰
            try:
                if os.path.exists(entities_file_path_txt):
                    with open(entities_file_path_txt, "r", encoding='utf-8') as f:
                        for line in f:
                            entity = line.strip()
                            if entity:
                                entities_list.append(entity)
                    print(f"ä»å®ä½“åˆ—è¡¨æ–‡ä»¶è¯»å–åˆ° {len(entities_list)} ä¸ªå®ä½“: {entities_file_path_txt}")
                else:
                    # å›é€€åˆ°csvæ–‡ä»¶ï¼ˆå®ä½“å…³ç³»ï¼‰
                    if os.path.exists(entities_file_path_csv):
                        with open(entities_file_path_csv, "r", encoding='utf-8') as csvfile:
                            csvreader = csv.reader(csvfile, delimiter=',')
                            for row in csvreader:
                                if len(row) >= 2:
                                    entities_list.append(row[0].strip())
                                    entities_list.append(row[1].strip())
                        print(f"ä»å®ä½“å…³ç³»æ–‡ä»¶è¯»å–åˆ° {len(entities_list)} ä¸ªå®ä½“: {entities_file_path_csv}")
                    else:
                        raise FileNotFoundError(f"Neither {entities_file_path_txt} nor {entities_file_path_csv} found")
            except Exception as e:
                print(f"âœ— é”™è¯¯ï¼šæ— æ³•è¯»å–å®ä½“æ–‡ä»¶: {e}")
                raise
            
            # æ ¹æ®å®ä½“æ–‡ä»¶ååˆ¤æ–­è¯­è¨€ï¼šmedqa/dart/triviaqaæ˜¯è‹±æ–‡ï¼Œå…¶ä»–é»˜è®¤ä¸­æ–‡
            language = "en" if any(x in entities_file_name.lower() for x in ["medqa", "dart", "triviaqa", "aeslc"]) else "zh"
            from entity.ruler import enhance_spacy
            self.nlp = enhance_spacy(entities_list, language=language)
            print(f"âœ“ Spacyæ¨¡å‹åŠ è½½å¹¶å¢å¼ºå®Œæˆï¼ˆå·²æ·»åŠ  {len(entities_list)} ä¸ªå®ä½“æ¨¡å¼ï¼‰")
            
            # åˆå§‹åŒ–Cuckoo Filter
            if hash.filter is None:
                hash.change_filter(len(entities_list) * 2)  # ä¹˜ä»¥2ä»¥ç•™å‡ºç©ºé—´
                print(f"âœ“ Cuckoo Filterå·²åˆå§‹åŒ–ï¼Œå®¹é‡: {len(entities_list) * 2}")
            
            # æ„å»ºAbstractTreeå’ŒEntityæ˜ å°„
            # é¦–å…ˆå°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
            import pickle
            abstract_cache_dir = "./abstract_forest_cache"
            os.makedirs(abstract_cache_dir, exist_ok=True)
            cache_file_path = f"{abstract_cache_dir}/abstract_forest_{self.vec_db_key}.pkl"
            
            if os.path.exists(cache_file_path):
                print(f"æ­£åœ¨ä»ç¼“å­˜åŠ è½½AbstractTree: {cache_file_path}")
                try:
                    with open(cache_file_path, 'rb') as f:
                        cached_data = pickle.load(f)
                        self.abstract_forest = cached_data['abstract_forest']
                        self.entity_to_abstract_map = cached_data['entity_to_abstract_map']
                        self.entity_abstract_address_map = cached_data['entity_abstract_address_map']
                    
                    # éªŒè¯ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæ£€æŸ¥å®ä½“æ•°é‡æ˜¯å¦åŒ¹é…ï¼‰
                    if len(entities_list) == cached_data.get('entities_count', 0):
                        print(f"âœ“ ä»ç¼“å­˜åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.abstract_forest)} ä¸ªAbstractTree")
                        # æ›´æ–°Cuckoo Filteræ˜ å°„ï¼ˆä»ç¼“å­˜ä¸­æ¢å¤çš„æ˜ å°„å¯èƒ½å·²è¿‡æœŸï¼‰
                        from trag_tree.set_cuckoo_abstract_addresses import set_entity_abstract_addresses_in_cuckoo
                        print("æ­£åœ¨æ›´æ–°Cuckoo Filteråœ°å€æ˜ å°„ï¼ˆä½¿ç”¨ç¼“å­˜çš„æ˜ å°„ï¼‰...")
                        for entity, pair_ids in self.entity_abstract_address_map.items():
                            if pair_ids:
                                set_entity_abstract_addresses_in_cuckoo(entity, pair_ids)
                        print(f"âœ“ å·²æ›´æ–° {len([e for e, ids in self.entity_abstract_address_map.items() if ids])} ä¸ªentitiesçš„Abstractåœ°å€æ˜ å°„åˆ°Cuckoo Filter")
                    else:
                        print(f"âš  ç¼“å­˜ä¸­çš„å®ä½“æ•°é‡ ({cached_data.get('entities_count', 0)}) ä¸å½“å‰å®ä½“æ•°é‡ ({len(entities_list)}) ä¸åŒ¹é…ï¼Œé‡æ–°æ„å»º")
                        raise ValueError("Cache mismatch")
                except Exception as e:
                    print(f"âš  åŠ è½½ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°æ„å»ºAbstractTree")
                    # ç»§ç»­æ‰§è¡Œæ„å»ºæµç¨‹
            
            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œé‡æ–°æ„å»º
            if not hasattr(self, 'abstract_forest') or self.abstract_forest is None:
                print("æ­£åœ¨æ„å»ºAbstractTreeå’ŒEntityæ˜ å°„ï¼ˆæ–°æ¶æ„ï¼‰...")
                try:
                    # è·å–table_name
                    from rag_base import build_index
                    db_id = id(self.vec_db)
                    table_name = None
                    if hasattr(build_index.load_vec_db, '_db_table_map'):
                        table_name = build_index.load_vec_db._db_table_map.get(db_id)
                    if table_name is None:
                        keys = self.vec_db.get_all_keys()
                        table_name = keys[0] if keys else "default_table"
                    
                    # æ„å»ºAbstractForestï¼ˆå¤šä¸ªAbstractTreeï¼‰å’Œæ˜ å°„
                    self.abstract_forest, self.entity_to_abstract_map, self.entity_abstract_address_map = build.build_abstract_forest_and_entity_mapping(
                        self.vec_db,
                        entities_list,
                        table_name=table_name
                    )
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    print(f"æ­£åœ¨ä¿å­˜AbstractTreeåˆ°ç¼“å­˜: {cache_file_path}")
                    try:
                        with open(cache_file_path, 'wb') as f:
                            pickle.dump({
                                'abstract_forest': self.abstract_forest,
                                'entity_to_abstract_map': self.entity_to_abstract_map,
                                'entity_abstract_address_map': self.entity_abstract_address_map,
                                'entities_count': len(entities_list)
                            }, f)
                        print(f"âœ“ AbstractTreeå·²ä¿å­˜åˆ°ç¼“å­˜")
                    except Exception as e:
                        print(f"âš  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}ï¼Œä½†ä¸å½±å“è¿è¡Œ")
                except Exception as e:
                    print(f"âœ— é”™è¯¯ï¼šæ„å»ºAbstractForestå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    raise
            
            # å°†æ•°æ®å­˜å‚¨åˆ°æ¨¡å—çº§å˜é‡ï¼Œä¾›rag_completeä½¿ç”¨
            from rag_base import rag_complete as rag_module
            rag_module._abstract_forest = self.abstract_forest  # ç°åœ¨æ˜¯åˆ—è¡¨
            rag_module._entity_to_abstract_map = self.entity_to_abstract_map
            rag_module._entity_abstract_address_map = self.entity_abstract_address_map  # Cuckoo Filteråœ°å€æ˜ å°„
            
            # ç»Ÿè®¡æ‰€æœ‰AbstractTreeä¸­çš„abstractsæ€»æ•°
            total_abstracts = sum(len(tree.get_all_nodes()) for tree in self.abstract_forest)
            print(f"âœ“ AbstractForestå·²å°±ç»ªï¼ŒåŒ…å« {len(self.abstract_forest)} ä¸ªAbstractTreeï¼Œå…± {total_abstracts} ä¸ªabstracts")
            print(f"âœ“ Cuckoo Filteråœ°å€æ˜ å°„å·²æ›´æ–°ï¼šEntityNodeåœ°å€å·²æ›¿æ¢ä¸ºAbstractNodeåœ°å€")
        else:
            # å…¶ä»–search_methodï¼šæ„å»ºforestå’Œnlp
            self.forest, self.nlp = build.build_forest(
                tree_num_max, entities_file_name, search_method, node_num_max
            )
            print(f"âœ“ Forestå’ŒNLPåŠ è½½å®Œæˆ ({time.time() - start_time:.2f}ç§’)")
            
            # æ ¹æ®search_methodæ‰§è¡Œä¸åŒçš„åˆå§‹åŒ–
            if search_method in [4, 8]:
                for entity_tree in self.forest:
                    entity_tree.bfs_hash()
            
            if search_method in [9]:
                from grag_graph.graph import build_graph
                build_graph(entities_file_name)
            
            if search_method in [8, 9]:
                from ann.ann_calc import build_ann
                build_ann()
        
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ ({time.time() - start_time:.2f}ç§’)\n")
    
    def evaluate(self, question: str, expected_answer: str = None) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        start_time = time.time()
        
        # å¯¼å…¥rag_completeæ¨¡å—ä»¥è·å–retrieval_timeå’Œgeneration_time
        from rag_base.rag_complete import get_retrieval_time, get_generation_time
        
        # è·å–å›ç­”
        stream = rag_complete(
            question,
            self.vec_db,
            self.forest,
            self.nlp,
            search_method=self.search_method,
            debug=False,
            max_hierarchy_depth=getattr(self, 'max_hierarchy_depth', 2),
        )
        
        answer = ""
        for chunk in stream:
            answer += chunk
        
        elapsed_time = time.time() - start_time
        
        # è·å–æ£€ç´¢æ—¶é—´å’Œç”Ÿæˆæ—¶é—´
        retrieval_time = get_retrieval_time()
        generation_time = get_generation_time()
        
        result = {
            "question": question,
            "answer": answer,
            "expected_answer": expected_answer,
            "time": elapsed_time,
            "answer_length": len(answer)
        }
        
        # å¦‚æœæ£€ç´¢æ—¶é—´å’Œç”Ÿæˆæ—¶é—´å¯ç”¨ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if retrieval_time is not None:
            result["retrieval_time"] = retrieval_time
        if generation_time is not None:
            result["generation_time"] = generation_time
        
        return result
    
    def run_dataset(self, dataset: List[Dict[str, str]], max_samples: int = None, 
                   checkpoint_path: str = None, resume: bool = True) -> List[Dict[str, Any]]:
        """åœ¨æ•°æ®é›†ä¸Šè¿è¡Œbenchmarkï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        
        Args:
            dataset: æ•°æ®é›†
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºä¿å­˜å’Œæ¢å¤è¿›åº¦ï¼‰
            resume: æ˜¯å¦ä»checkpointæ¢å¤
        """
        if max_samples:
            dataset = dataset[:max_samples]
        
        # å°è¯•ä»checkpointæ¢å¤
        completed_questions = set()
        results = []
        start_idx = 0
        
        if resume and checkpoint_path and os.path.exists(checkpoint_path):
            try:
                print(f"æ­£åœ¨ä»checkpointæ¢å¤: {checkpoint_path}")
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    if isinstance(checkpoint_data, list):
                        results = checkpoint_data
                        # è®°å½•å·²å®Œæˆçš„é—®é¢˜ï¼ˆä½¿ç”¨å®Œæ•´questionä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰
                        completed_questions = {r['question'] for r in results if r.get('question')}
                        start_idx = len(results)
                        print(f"âœ“ ä»checkpointæ¢å¤äº† {len(results)} ä¸ªå·²å®Œæˆçš„ç»“æœ")
            except Exception as e:
                print(f"âš  è¯»å–checkpointå¤±è´¥: {e}ï¼Œä»å¤´å¼€å§‹è¿è¡Œ")
                results = []
                completed_questions = set()
                start_idx = 0
        
        total_start = time.time()
        
        print(f"\nå¼€å§‹æµ‹è¯• {len(dataset)} ä¸ªé—®é¢˜...")
        if start_idx > 0:
            print(f"å·²å®Œæˆ {start_idx} ä¸ªï¼Œå‰©ä½™ {len(dataset) - start_idx} ä¸ª")
        print("=" * 80)
        
        checkpoint_interval = 10  # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡checkpoint
        
        for i, item in enumerate(dataset[start_idx:], start_idx + 1):
            question = item.get("prompt", item.get("question", ""))
            expected = item.get("answer", item.get("expected_answer", ""))
            
            if not question:
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆé¿å…é‡å¤è¿è¡Œï¼‰
            # ä½¿ç”¨å®Œæ•´questionä½œä¸ºæ ‡è¯†ï¼Œå› ä¸ºå¾ˆå¤šé—®é¢˜çš„å‰100å­—ç¬¦å¯èƒ½ç›¸åŒ
            question_key = question  # ä½¿ç”¨å®Œæ•´questionè€Œä¸æ˜¯å‰100å­—ç¬¦
            if question_key in completed_questions:
                print(f"\n[{i}/{len(dataset)}] â­ è·³è¿‡å·²å®Œæˆçš„: {question[:60]}...")
                continue
            
            print(f"\n[{i}/{len(dataset)}] {question[:60]}...")
            
            try:
                result = self.evaluate(question, expected)
                results.append(result)
                completed_questions.add(question_key)
                
                print(f"  å›ç­”é•¿åº¦: {len(result['answer'])} å­—ç¬¦")
                print(f"  è€—æ—¶: {result['time']:.2f}ç§’")
                
                # å®šæœŸä¿å­˜checkpoint
                if checkpoint_path and i % checkpoint_interval == 0:
                    try:
                        os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                        with open(checkpoint_path, 'w', encoding='utf-8') as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)
                        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜ ({i}/{len(dataset)})")
                    except Exception as e:
                        print(f"  âš  Checkpointä¿å­˜å¤±è´¥: {e}")
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
                # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼Œä½†æ ‡è®°ä¸ºå¤±è´¥
                results.append({
                    "question": question,
                    "answer": f"[ERROR: {str(e)}]",
                    "expected_answer": expected,
                    "time": 0,
                    "answer_length": 0,
                    "error": str(e)
                })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        
        total_time = time.time() - total_start
        
        # æœ€ç»ˆä¿å­˜checkpoint
        if checkpoint_path:
            try:
                os.makedirs(os.path.dirname(checkpoint_path) or '.', exist_ok=True)
                with open(checkpoint_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ æœ€ç»ˆcheckpointå·²ä¿å­˜: {checkpoint_path}")
            except Exception as e:
                print(f"âš  æœ€ç»ˆcheckpointä¿å­˜å¤±è´¥: {e}")
        
        print("\n" + "=" * 80)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        print(f"æ€»é—®é¢˜æ•°: {len(results)}")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        if results:
            avg_time = sum(r['time'] for r in results) / len(results)
            avg_length = sum(r['answer_length'] for r in results) / len(results)
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’")
            print(f"å¹³å‡å›ç­”é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print("=" * 80)
        
        return results
    
    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def load_json_dataset(file_path: str) -> List[Dict[str, str]]:
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆåŒ…å«å¤šä¸ªæ•°æ®é›†ï¼‰ï¼Œæå–ç¬¬ä¸€ä¸ª
    if isinstance(data, dict):
        # å°è¯•æ‰¾åˆ°åˆ—è¡¨æ ¼å¼çš„æ•°æ®
        for key, value in data.items():
            if isinstance(value, list):
                return value
        return []
    
    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if isinstance(data, list):
        return data
    
    return []


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¿è¡Œbenchmarkæµ‹è¯•")
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vec-db-key', type=str, default="test",
                       help='å‘é‡æ•°æ®åº“key')
    parser.add_argument('--tree-num-max', type=int, default=50,
                       help='æœ€å¤§æ ‘æ•°é‡')
    parser.add_argument('--entities-file-name', type=str, default="entities_file",
                       help='å®ä½“æ–‡ä»¶åï¼ˆä¸å«.csvæ‰©å±•åï¼‰')
    parser.add_argument('--search-method', type=int, default=0,
                       choices=[0, 1, 2, 5, 7, 8, 9],
                       help='æœç´¢æ–¹æ³•: 0 for vec-db only (standard RAG), 1 for BFS, 2 for BloomFilter, 5 for improved BloomFilter, 7 for Cuckoo Filter, 8 for ANN-Tree, 9 for ANN-Graph')
    parser.add_argument('--node-num-max', type=int, default=2000000,
                       help='æœ€å¤§èŠ‚ç‚¹æ•°')
    parser.add_argument('--max-hierarchy-depth', type=int, default=2,
                       help='æœ€å¤§å±‚æ¬¡æ·±åº¦ï¼ˆç”¨äºCuckoo Filter Abstract RAGï¼‰')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='ç»“æœè¾“å‡ºè·¯å¾„')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='Checkpointæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼Œé»˜è®¤ä¸outputç›¸åŒï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»checkpointæ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†: {args.dataset}")
    dataset = load_json_dataset(args.dataset)
    
    if not dataset:
        print(f"âœ— æ— æ³•åŠ è½½æ•°æ®é›†: {args.dataset}")
        return
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ•°æ®\n")
    
    # åˆ›å»ºrunner
    runner = BenchmarkRunner(
        vec_db_key=args.vec_db_key,
        tree_num_max=args.tree_num_max,
        entities_file_name=args.entities_file_name,
        search_method=args.search_method,
        node_num_max=args.node_num_max,
        max_hierarchy_depth=args.max_hierarchy_depth
    )
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„å’Œcheckpointè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºè·¯å¾„
        dataset_name = Path(args.dataset).stem
        output_path = f"./benchmark/results/{dataset_name}_results_{args.search_method}.json"
    
    checkpoint_path = args.checkpoint if args.checkpoint else output_path
    resume = not args.no_resume
    
    # è¿è¡Œæµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    results = runner.run_dataset(
        dataset, 
        max_samples=args.max_samples,
        checkpoint_path=checkpoint_path,
        resume=resume
    )
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    runner.save_results(results, output_path)


if __name__ == "__main__":
    main()

